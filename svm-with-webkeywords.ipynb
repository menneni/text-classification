{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = \"based private ltd co serive services amp /n /r ltd provide year company special  giving established various became 1987 range like every center best quality shop india indian complete range leading concern like time latest every one well-known also south delhi mumbai india indian bangalore hyderabad chennai\"\n",
    "stoplist += more_stopwords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'i'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import sparse\n",
    "\n",
    "def loadData(filePath=\"dataset.csv\"):\n",
    "    data=pd.read_csv(\"/home/administrator/data/categories-data/Train-Data/fps-with-cat-train.csv\") \n",
    "    data['CategoryFB'] = data['CategoryFB'].fillna(data['CategoryV2'])\n",
    "    #data = data.dropna(subset=['Description'])\n",
    "    #data = data.reset_index(drop=True)\n",
    "    return data[\"Tag\"],data[\"Name\"],data[\"CategoryV2\"]\n",
    "\n",
    "def preProcessing(features):\n",
    "    num_descs = features.size\n",
    "    clean_wordlist = []\n",
    "    clean_descs = []\n",
    "    stops = set(stopwords.words('english'))\n",
    "    more_stopwords = \" since private ltd co based nwe n r \\n \\r amp st ncar nagar nto nand quot k provide service services year giving established various became 1987 range like every center best quality india indian complete range leading concern like time latest every one well-known also south delhi mumbai india indian bangalore hyderabad chennai\"\n",
    "    stops.update(more_stopwords.split())\n",
    "    for i in range( 0, num_descs):\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", features[i])\n",
    "        #print letters_only\n",
    "        words = letters_only.lower().split()\n",
    "        words = [w.lower() for w in words if not w in stops]  \n",
    "        clean_wordlist.append(words)\n",
    "        clean_descs.append(\" \".join(words))\n",
    "    return clean_descs, clean_wordlist\n",
    "\n",
    "\n",
    "def getDTMByTFIDF(features,nfeatures):\n",
    "    tfIdf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df = 0, stop_words = 'english', max_features=nfeatures)\n",
    "    dtm = tfIdf_vectorizer.fit_transform(features)\n",
    "    return dtm,tfIdf_vectorizer\n",
    "\n",
    "def featuresByChiSq(features,labels,nFeature=3000):\n",
    "    chi2_model = SelectKBest(chi2,k=nFeature)\n",
    "    dtm = chi2_model.fit_transform(features,labels)\n",
    "    return dtm,chi2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_search\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='linear', probability=True, class_weight='auto'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "\n",
    "def crossValidate(document_term_matrix,labels,classifier=\"SVM\",nfold=10):\n",
    "    clf = None\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fscore = []\n",
    "    \n",
    "    if classifier == \"RF\":\n",
    "        clf = RandomForestClassifier(class_weight='auto')\n",
    "    elif classifier == \"NB\":\n",
    "        clf = MultinomialNB()\n",
    "    elif classifier == \"SVM\":\n",
    "        clf = LinearSVC(class_weight='auto')\n",
    "        #best_params = svc_param_selection(document_term_matrix, labels, nfold)\n",
    "        #print best_params\n",
    "        \n",
    "    \n",
    "    skf = StratifiedKFold(labels, n_folds=nfold)\n",
    "\n",
    "    for train_index, test_index in skf:\n",
    "        X_train, X_test = document_term_matrix[train_index], document_term_matrix[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        model = clf.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        p,r,f,s = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        fscore.append(f)\n",
    "        \n",
    "    return round(np.mean(precision),3),round(np.mean(recall),3),round(np.mean(fscore),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hanuman Enterprises India Pvt. Ltd.\n"
     ]
    }
   ],
   "source": [
    "tags, descs, labels = loadData()\n",
    "print descs[0]\n",
    "processed_descs, processed_descs_wordlist = preProcessing(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34492"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm,vect = getDTMByTFIDF(processed_descs,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisqDtm, chisqModel = featuresByChiSq(dtm,labels,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, fscore = crossValidate(chisqDtm,labels,\"SVM\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941 0.867 <function precision_recall_fscore_support at 0x7fe7bce175f0>\n"
     ]
    }
   ],
   "source": [
    "print precision, recall, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_descs, test_descs = train_test_split(descs, test_size=0.1, \n",
    "                                           random_state=42)\n",
    "\n",
    "train_labels, test_labels = train_test_split(labels, test_size=0.1, \n",
    "                                           random_state=42)\n",
    "\n",
    "train_descs = train_descs.reset_index(drop=True)\n",
    "test_descs = test_descs.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "processed__train_descs, processed_train_descs_wordlist = preProcessing(train_descs)\n",
    "processed_test_descs, processed_test_descs_wordlist = preProcessing(test_descs)\n",
    "\n",
    "dtm_train,vect_train = getDTMByTFIDF(processed__train_descs,2000)\n",
    "chisqDtmTrain, chisqModelTrain = featuresByChiSq(dtm_train,train_labels,2000)\n",
    "\n",
    "clf = RandomForestClassifier(class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_descs, test_descs = train_test_split(descs, test_size=0.1, \n",
    "                                           random_state=42)\n",
    "\n",
    "train_labels, test_labels = train_test_split(labels, test_size=0.1, \n",
    "                                           random_state=42)\n",
    "\n",
    "train_descs = train_descs.reset_index(drop=True)\n",
    "test_descs = test_descs.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "processed__train_descs, processed_train_descs_wordlist = preProcessing(train_descs)\n",
    "processed_test_descs, processed_test_descs_wordlist = preProcessing(test_descs)\n",
    "\n",
    "dtm_train,vect_train = getDTMByTFIDF(processed__train_descs,1500)\n",
    "chisqDtmTrain, chisqModelTrain = featuresByChiSq(dtm_train,train_labels,1000)\n",
    "\n",
    "clf = LinearSVC(class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(chisqDtmTrain, train_labels)\n",
    "dtm_test,vect_test = getDTMByTFIDF(processed_test_descs,1500)\n",
    "chisqDtmTest, chisqModelTest = featuresByChiSq(dtm_test,test_labels,1000)\n",
    "y_pred = model.predict(chisqDtmTest)\n",
    "#predictions = model.predict_proba(chisqDtmTest)\n",
    "\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(test_labels, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.239505684564 0.132753623188 0.165201961609 None\nBEAUTY & SPA\nTOURISM\nKouser Travels\n"
     ]
    }
   ],
   "source": [
    "print p,r,f,s\n",
    "\n",
    "print y_pred[245]\n",
    "\n",
    "print test_labels[245]\n",
    "\n",
    "print test_descs[245]\n",
    "\n",
    "#labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cdict = defaultdict(list)\n",
    "for i, val in enumerate(labels):\n",
    "    if val in cdict:\n",
    "        cdict[val].extend(processed_descs_wordlist[i])\n",
    "    else:\n",
    "        value = list()\n",
    "        cdict[val] = processed_descs_wordlist[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty: 930\nsalon: 709\nspa: 285\nparlour: 232\nnaturals: 158\nunisex: 144\nhair: 94\ncare: 94\nfamily: 56\nclinic: 51\nsaloon: 47\nladies: 45\nproducts: 36\nstudio: 33\nacademy: 32\nherbal: 31\ncentre: 29\nroad: 28\nelegance: 27\npvt: 26\nfantasy: 26\nnikhar: 23\ntraining: 22\nhealth: 22\nlounge: 21\navon: 21\nparlor: 20\nmake: 17\nzone: 16\nspas: 16\nlakme: 16\nworld: 15\nnew: 15\njailaxmi: 15\nsaloni: 14\nfashion: 14\nwomen: 12\nwellness: 11\nmakeup: 11\nday: 11\nthai: 10\nstyle: 10\nsri: 10\nskin: 10\nprofessional: 10\nombre: 10\ncollection: 10\naura: 10\nanokhi: 10\nsenhora: 9\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "fq= defaultdict( int )\n",
    "for w in cdict['BEAUTY & SPA']:\n",
    "    fq[w] += 1\n",
    "    \n",
    "copy = []\n",
    "for k,v in fq.items():\n",
    "    copy.append((v, k))\n",
    "\n",
    "\n",
    "copy = sorted(copy, reverse=True)\n",
    "\n",
    "index = 0\n",
    "for k in copy:\n",
    "    if index == 50:\n",
    "        break\n",
    "    else:\n",
    "        print '%s: %d' %(k[1], k[0])\n",
    "    index = index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n\n           ARCHITECTURE       0.00      0.00      0.00         7\n                   ARTS       0.00      0.00      0.00         4\n             AUTOMOTIVE       0.00      0.00      0.00        14\n           BEAUTY & SPA       0.30      0.19      0.23       175\n                  BLOGS       0.01      0.15      0.02        13\n               CATERING       0.00      0.00      0.00         2\n              CHEMICALS       0.00      0.00      0.00         4\n           CONSTRUCTION       0.10      0.33      0.15         3\n              EDUCATION       0.11      0.11      0.11       417\n            ELECTRONICS       0.64      0.57      0.60       441\n          ENTERTAINMENT       0.00      0.00      0.00         8\n                  EVENT       0.00      0.00      0.00        20\n                FASHION       0.35      0.13      0.19       311\n                FLORIST       0.00      0.00      0.00         2\n       FOOD & BEVERAGES       0.03      0.03      0.03       120\n              FURNITURE       0.02      0.01      0.02       145\n                  GIFTS       0.00      0.00      0.00         3\n                GROCERY       0.00      0.00      0.00         0\nHARDWARE & SANITARYWARE       0.01      0.01      0.01       110\n                 HEALTH       0.04      0.25      0.07        12\n        HOME APPLIANCES       0.03      0.17      0.06         6\n              HOME CARE       0.00      0.00      0.00         8\n       HOME MAINTENANCE       0.00      0.00      0.00        85\n                  HOTEL       0.20      0.12      0.15       295\n                HOUSING       0.00      0.00      0.00         2\n             INDIVIDUAL       0.00      0.00      0.00         8\n        INTERIOR DESIGN       0.00      0.00      0.00         7\n              JEWELLERY       0.00      0.00      0.00        96\n                   KIDS       0.00      0.00      0.00        14\n                KITCHEN       0.00      0.00      0.00         2\n          MANUFACTURING       0.06      0.04      0.05        24\n                MARBLES       0.00      0.00      0.00         2\n                MEDICAL       0.47      0.29      0.36       444\n                   PETS       0.00      0.00      0.00         3\n            PHOTOGRAPHY       0.00      0.00      0.00        55\n              PROMOTION       0.00      0.00      0.00         5\n            REAL ESTATE       0.03      0.03      0.03        69\n                 RETAIL       0.00      0.00      0.00         7\n               SECURITY       0.06      0.03      0.04        64\n                 SPORTS       0.79      0.63      0.70        67\n    TECHNOLOGY SERVICES       0.00      0.00      0.00         4\n                TOURISM       0.42      0.42      0.42       370\n                TRADING       0.00      0.00      0.00         2\n\n            avg / total       0.28      0.22      0.24      3450\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print classification_report(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}